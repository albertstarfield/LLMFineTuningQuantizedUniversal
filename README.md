# LLMFineTuningQuantizedUniversal
LLM Finetuning while saving memory without using Nvidia, Intel x86 Exclusive, AMD ROCm, Unsloth, BitsandBytes and convert back into gguf using pytorch
